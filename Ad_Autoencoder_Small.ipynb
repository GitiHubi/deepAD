{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## \"Detection of Anomalies in Financial Transactions using Deep Autoencoder Networks\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of this \"GPU Technology Conference (GTC) 2018, Silicon Valley\" lab was jointly developed by Marco Schreyer and Timur Sattarov. Please don't hesitate to contact us in case of any questions via <a href=\"mailto:marco.schreyer@dfki.de\">marco.schreyer@dfki.de</a> and <a href=\"mailto:sattarov.timur@pwc.com\">sattarov.timur@pwc.com</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to ask :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Python Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's verify that Python is working on your system. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Shift-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer should be forty-two: 42\n"
     ]
    }
   ],
   "source": [
    "print('The answer should be forty-two: {}'.format(str(40+2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Python Libraries Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step let's import the libraries needed throughout the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing utilities\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# importing pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CUDNN and GPU Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if CDNN is available on the server let's execute the cell below to display information about the available CUDNN version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180511-15:23:15] The CUDNN backend version: None\n"
     ]
    }
   ],
   "source": [
    "# print CUDNN backend version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] The CUDNN backend version: {}'.format(now, torch.backends.cudnn.version()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's display information about the potential GPUs running on the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CUDNN and GPU's are available let's still specify if we want to use both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Python and PyTorch Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the Python and PyTorch version running on the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180511-15:23:16] The Python version: 2.7.14 (default, Feb  1 2018, 16:41:55) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)]\n"
     ]
    }
   ],
   "source": [
    "# print current Python version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] The Python version: {}'.format(now, sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180511-15:23:16] The PyTorch version: 0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "# print current PyTorch version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] The PyTorch version: {}'.format(now, torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Random Seed Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let' set the seeds of random elements in the code e.g. the initialization of the network parameters to guarantee deterministic computation and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234 #4444 #3333 #2222 #1111 #1234\n",
    "rd.seed(seed_value) # set random seed\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    torch.cuda.manual_seed(seed_value) # set pytorch seed GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Financial Fraud Detection Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will conduct a descriptive analysis of the labs financial dataset. Furthermore, we will apply some necessary pre-processing steps to train a deep neural network. The lab is based on a derivation of the **\"Synthetic Financial Dataset For Fraud Detection\"** by Lopez-Rojas [6] available via the Kaggle predictive modelling and analytics competitions platform that can be obtained using the following link: https://www.kaggle.com/ntnu-testimon/paysim1.\n",
    "\n",
    "Let's start loading the dataset and investigate its structure and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset into the notebook kernel\n",
    "ori_dataset = pd.read_csv('./data/fraud_dataset_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180511-15:23:18] Transactional dataset of 533009 rows and 10 columns loaded\n"
     ]
    }
   ],
   "source": [
    "# inspect the datasets dimensionalities\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] Transactional dataset of {} rows and {} columns loaded'.format(now, ori_dataset.shape[0], ori_dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initial Data and Attribute Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We augmented the dataset and renamed the attributes to appear more similar to a real-world dataset that one usually observes in SAP-ERP systems as part of SAP's Finance and Cost controlling (FICO) module. \n",
    "\n",
    "The dataset contains a subset of in total 7 categorical and 2 numerical attributes available in the FICO BKPF (containing the posted journal entry headers) and BSEG (containing the posted journal entry segments) tables. Please, find below a list of the individual attributes as well as a brief description of their respective semantics:\n",
    "\n",
    ">- `BELNR`: the accounting document number,\n",
    ">- `BUKRS`: the company code,\n",
    ">- `BSCHL`: the posting key,\n",
    ">- `HKONT`: the posted general ledger account,\n",
    ">- `PRCTR`: the posted profit center,\n",
    ">- `WAERS`: the currency key,\n",
    ">- `KTOSL`: the general ledger account key,\n",
    ">- `DMBTR`: the amount in local currency,\n",
    ">- `WRBTR`: the amount in document currency.\n",
    "\n",
    "Let's also have a closer look into the top 10 rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BELNR</th>\n",
       "      <th>WAERS</th>\n",
       "      <th>BUKRS</th>\n",
       "      <th>KTOSL</th>\n",
       "      <th>PRCTR</th>\n",
       "      <th>BSCHL</th>\n",
       "      <th>HKONT</th>\n",
       "      <th>DMBTR</th>\n",
       "      <th>WRBTR</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>288203</td>\n",
       "      <td>C3</td>\n",
       "      <td>C31</td>\n",
       "      <td>C9</td>\n",
       "      <td>C92</td>\n",
       "      <td>A3</td>\n",
       "      <td>B1</td>\n",
       "      <td>280979.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324441</td>\n",
       "      <td>C1</td>\n",
       "      <td>C18</td>\n",
       "      <td>C7</td>\n",
       "      <td>C76</td>\n",
       "      <td>A1</td>\n",
       "      <td>B2</td>\n",
       "      <td>129856.53</td>\n",
       "      <td>243343.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133537</td>\n",
       "      <td>C1</td>\n",
       "      <td>C19</td>\n",
       "      <td>C2</td>\n",
       "      <td>C20</td>\n",
       "      <td>A1</td>\n",
       "      <td>B3</td>\n",
       "      <td>957463.97</td>\n",
       "      <td>3183838.41</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>331521</td>\n",
       "      <td>C4</td>\n",
       "      <td>C48</td>\n",
       "      <td>C9</td>\n",
       "      <td>C95</td>\n",
       "      <td>A2</td>\n",
       "      <td>B1</td>\n",
       "      <td>2681709.51</td>\n",
       "      <td>28778.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375333</td>\n",
       "      <td>C5</td>\n",
       "      <td>C58</td>\n",
       "      <td>C1</td>\n",
       "      <td>C19</td>\n",
       "      <td>A3</td>\n",
       "      <td>B1</td>\n",
       "      <td>910514.49</td>\n",
       "      <td>346.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>327203</td>\n",
       "      <td>C1</td>\n",
       "      <td>C15</td>\n",
       "      <td>C6</td>\n",
       "      <td>C68</td>\n",
       "      <td>A1</td>\n",
       "      <td>B2</td>\n",
       "      <td>357627.56</td>\n",
       "      <td>704520.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>292545</td>\n",
       "      <td>C4</td>\n",
       "      <td>C47</td>\n",
       "      <td>C2</td>\n",
       "      <td>C28</td>\n",
       "      <td>A2</td>\n",
       "      <td>B3</td>\n",
       "      <td>955576.84</td>\n",
       "      <td>128328.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>335839</td>\n",
       "      <td>C1</td>\n",
       "      <td>C19</td>\n",
       "      <td>C1</td>\n",
       "      <td>C17</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>41769.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>369064</td>\n",
       "      <td>C4</td>\n",
       "      <td>C40</td>\n",
       "      <td>C9</td>\n",
       "      <td>C97</td>\n",
       "      <td>A2</td>\n",
       "      <td>B1</td>\n",
       "      <td>44309.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>138724</td>\n",
       "      <td>C6</td>\n",
       "      <td>C69</td>\n",
       "      <td>C1</td>\n",
       "      <td>C12</td>\n",
       "      <td>A2</td>\n",
       "      <td>B1</td>\n",
       "      <td>466720.45</td>\n",
       "      <td>43843.00</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BELNR WAERS BUKRS KTOSL PRCTR BSCHL HKONT       DMBTR       WRBTR    label\n",
       "0  288203    C3   C31    C9   C92    A3    B1   280979.60        0.00  regular\n",
       "1  324441    C1   C18    C7   C76    A1    B2   129856.53   243343.00  regular\n",
       "2  133537    C1   C19    C2   C20    A1    B3   957463.97  3183838.41  regular\n",
       "3  331521    C4   C48    C9   C95    A2    B1  2681709.51    28778.00  regular\n",
       "4  375333    C5   C58    C1   C19    A3    B1   910514.49      346.00  regular\n",
       "5  327203    C1   C15    C6   C68    A1    B2   357627.56   704520.00  regular\n",
       "6  292545    C4   C47    C2   C28    A2    B3   955576.84   128328.00  regular\n",
       "7  335839    C1   C19    C1   C17    A1    B1    41769.26        0.00  regular\n",
       "8  369064    C4   C40    C9   C97    A2    B1    44309.79        0.00  regular\n",
       "9  138724    C6   C69    C1   C12    A2    B1   466720.45    43843.00  regular"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect top rows of dataset\n",
    "ori_dataset.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also have noticed the attribute `label` in the data. We will use this field throughout the lab to evaluate the quality of our trained models. The field describes the true nature of each individual transaction of either being a **regular** transaction (denoted by `regular`) or an **anomaly** (denoted by `global` and `local`). Let's have closer look into the distribution of the regular vs. anomalous transactions in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regular    532909\n",
       "global         70\n",
       "local          30\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of anomalies vs. regular transactions\n",
    "ori_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the statistic reveals that, similar to real world scenarios, we are facing a highly \"unbalanced\" dataset. Overall, the dataset contains only a small fraction of **100 (0.018%)** anomalous transactions. While the 100 anomalous entries encompass **70 (0.013%)** \"global\" anomalies and **30 (0.005%)** \"local\" anomalies as introduced in section 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"ground-truth\" label information for the following steps of the lab\n",
    "label = ori_dataset.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pre-Processing of Categorical Transaction Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial data assessment above we can observe that the majority of attributes recorded in AIS- and ERP-systems correspond to categorical (discrete) attribute values, e.g. the posting date, the general-ledger account, the posting type, the currency. Let's have a more detailed look into the distribution of two dataset attributes, namely (1) the posting key `BSCHL` as well as (2) the general ledger account `HKONT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x105641fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103151850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare to plot posting key and general ledger account side by side\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "# plot the distribution of the posting key attribute\n",
    "g = sns.countplot(x=ori_dataset['BSCHL'], ax=ax[0])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "g.set_title('Distribution of BSCHL attribute values')\n",
    "\n",
    "# plot the distribution of the general ledger account attribute\n",
    "g = sns.countplot(x=ori_dataset['HKONT'], ax=ax[1])\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "g.set_title('Distribution of HKONT attribute values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, neural networks are in general not designed to be trained directly on categorical data and require the attributes to be trained on to be numeric. One simple way to meet this requirement is by applying a technique referred to as **\"one-hot\" encoding**. Using this encoding technique, we will derive a numerical representation of each of the categorical attribute values. One-hot encoding creates new binary columns for each categorical attribute value present in the original data. \n",
    "\n",
    "Let's work through a brief example: The **categorical attribute Receiver** below contains the names \"John\", \"Timur\" and \"Marco\". We \"one-hot\" encode the names by creating a separate binary column for each possible name value observable in the \"Receiver\" column. Now, we encode for each transaction that contains the value \"John\" in the \"Receiver\" column this observation with 1.0 in the newly created \"John\" column and 0.0 in all other created name columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 430px; height: auto\" src=\"images/encoding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this technique will \"one-hot\" encode the 6 categorical attributes in the original transactional dataset. This can be achieved using the `get_dummies()` function available in the Pandas data science library:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical attributes to be \"one-hot\" encoded\n",
    "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT', 'BUKRS', 'WAERS']\n",
    "\n",
    "# encode categorical attributes into a binary one-hot encoded representation \n",
    "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's inspect the encoding of 10 sample transactions to see if we have been successfull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KTOSL_A19</th>\n",
       "      <th>KTOSL_B06</th>\n",
       "      <th>KTOSL_B12</th>\n",
       "      <th>KTOSL_B83</th>\n",
       "      <th>KTOSL_C1</th>\n",
       "      <th>KTOSL_C2</th>\n",
       "      <th>KTOSL_C3</th>\n",
       "      <th>KTOSL_C39</th>\n",
       "      <th>KTOSL_C4</th>\n",
       "      <th>KTOSL_C5</th>\n",
       "      <th>...</th>\n",
       "      <th>WAERS_U72</th>\n",
       "      <th>WAERS_U94</th>\n",
       "      <th>WAERS_V89</th>\n",
       "      <th>WAERS_W25</th>\n",
       "      <th>WAERS_W59</th>\n",
       "      <th>WAERS_X26</th>\n",
       "      <th>WAERS_Y59</th>\n",
       "      <th>WAERS_Z06</th>\n",
       "      <th>WAERS_Z37</th>\n",
       "      <th>WAERS_Z54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  616 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   KTOSL_A19  KTOSL_B06  KTOSL_B12  KTOSL_B83  KTOSL_C1  KTOSL_C2  KTOSL_C3  \\\n",
       "0          0          0          0          0         0         0         0   \n",
       "1          0          0          0          0         0         0         0   \n",
       "2          0          0          0          0         0         1         0   \n",
       "3          0          0          0          0         0         0         0   \n",
       "4          0          0          0          0         1         0         0   \n",
       "5          0          0          0          0         0         0         0   \n",
       "6          0          0          0          0         0         1         0   \n",
       "7          0          0          0          0         1         0         0   \n",
       "8          0          0          0          0         0         0         0   \n",
       "9          0          0          0          0         1         0         0   \n",
       "\n",
       "   KTOSL_C39  KTOSL_C4  KTOSL_C5    ...      WAERS_U72  WAERS_U94  WAERS_V89  \\\n",
       "0          0         0         0    ...              0          0          0   \n",
       "1          0         0         0    ...              0          0          0   \n",
       "2          0         0         0    ...              0          0          0   \n",
       "3          0         0         0    ...              0          0          0   \n",
       "4          0         0         0    ...              0          0          0   \n",
       "5          0         0         0    ...              0          0          0   \n",
       "6          0         0         0    ...              0          0          0   \n",
       "7          0         0         0    ...              0          0          0   \n",
       "8          0         0         0    ...              0          0          0   \n",
       "9          0         0         0    ...              0          0          0   \n",
       "\n",
       "   WAERS_W25  WAERS_W59  WAERS_X26  WAERS_Y59  WAERS_Z06  WAERS_Z37  WAERS_Z54  \n",
       "0          0          0          0          0          0          0          0  \n",
       "1          0          0          0          0          0          0          0  \n",
       "2          0          0          0          0          0          0          0  \n",
       "3          0          0          0          0          0          0          0  \n",
       "4          0          0          0          0          0          0          0  \n",
       "5          0          0          0          0          0          0          0  \n",
       "6          0          0          0          0          0          0          0  \n",
       "7          0          0          0          0          0          0          0  \n",
       "8          0          0          0          0          0          0          0  \n",
       "9          0          0          0          0          0          0          0  \n",
       "\n",
       "[10 rows x 616 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect encoded sample transactions\n",
    "ori_dataset_categ_transformed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Merge Categorical and Numerical Transaction Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge both pre-processed numerical and categorical attributes into a single dataset that we will use for training our deep autoencoder neural network (explained an implemented in the following section 4.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge categorical and numeric subsets\n",
    "ori_subset_transformed = ori_dataset_categ_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's again have a look at the dimensionality of the dataset after we applied the distinct pre-processing steps to the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533009, 616)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect final dimensions of pre-processed transactional data\n",
    "ori_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, upon completion of all the pre-processing steps (incl. the exercises) we should end up with an encoded dataset consisting of a total number of 533,009 records (rows) and **616 encoded attributes** (columns). Let's keep the number number of columns in mind since it will define the dimensionality of the input- and output-layer of our deep autoencoder network which we will now implement in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adversarial Autoencoder Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Autoencoder Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Adversarial Autoencoder Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the encoder network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the encoder network\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(encoder, self).__init__()\n",
    "        \n",
    "        # init dropout layer with probability p\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "        # specify layer 1 - in 616, out 512\n",
    "        self.encoder_L1 = nn.Linear(in_features=616, out_features=512, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform(self.encoder_L1.weight) # init weights according to [9]\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify layer 2 - in 512, out 3\n",
    "        self.encoder_L2 = nn.Linear(512, 3, bias=True)\n",
    "        nn.init.xavier_uniform(self.encoder_L2.weight)\n",
    "        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
    "        x = self.encoder_R2(self.encoder_L2(x)) # don't apply dropout to the AE bottleneck\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init and push to CUDNN / GPU if avalable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init training network classes / architectures\n",
    "encoder_train = encoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    encoder_train = encoder().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate correct initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180511-15:24:45] encoder architecture:\n",
      "\n",
      "encoder(\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      "  (encoder_L1): Linear(in_features=616, out_features=512)\n",
      "  (encoder_R1): LeakyReLU(0.4, inplace)\n",
      "  (encoder_L2): Linear(in_features=512, out_features=3)\n",
      "  (encoder_R2): LeakyReLU(0.4, inplace)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the decoder network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the decoder network\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(decoder, self).__init__()\n",
    "        \n",
    "        # init dropout layer with probability p\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "        # specify layer 1 - in 3, out 512\n",
    "        self.decoder_L1 = nn.Linear(in_features=3, out_features=512, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform(self.decoder_L1.weight)  # init weights according to [9]\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify layer 2 - in 512, out 616\n",
    "        self.decoder_L2 = nn.Linear(512, 616, bias=True)\n",
    "        nn.init.xavier_uniform(self.decoder_L2.weight)\n",
    "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
    "        x = self.decoder_R2(self.decoder_L2(x)) # don't apply dropout to the AE output\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init and push to CUDNN / GPU if avalable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init training network classes / architectures\n",
    "decoder_train = decoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    decoder_train = decoder().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate correct initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180511-15:26:16] decoder architecture:\n",
      "\n",
      "decoder(\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      "  (decoder_L1): Linear(in_features=3, out_features=512)\n",
      "  (decoder_R1): LeakyReLU(0.4, inplace)\n",
      "  (decoder_L2): Linear(in_features=512, out_features=616)\n",
      "  (decoder_R2): LeakyReLU(0.4, inplace)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the discriminator network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the discriminator network\n",
    "class discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(discriminator, self).__init__()\n",
    "\n",
    "        # init dropout layer with probability p\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "        # specify layer 1 - in 3, out 512\n",
    "        self.discriminator_L1 = nn.Linear(3, 512)\n",
    "        nn.init.xavier_uniform(self.discriminator_L1.weight)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify layer 2 - in 512, out 1\n",
    "        self.discriminator_L2 = nn.Linear(512, 1)\n",
    "        nn.init.xavier_uniform(self.discriminator_L2.weight)\n",
    "        self.sigmo1 = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu1(self.dropout(self.discriminator_L1(x)))\n",
    "        discrimination = self.sigmo1(self.discriminator_L2(x))\n",
    "\n",
    "        return discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init and push to CUDNN / GPU if avalable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init training network classes / architectures\n",
    "discriminator_train = discriminator()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    discriminator_train = discriminator().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate correct initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20180511-15:28:20] discriminator architecture:\n",
      "\n",
      "discriminator(\n",
      "  (dropout): Dropout(p=0.0, inplace)\n",
      "  (discriminator_L1): Linear(in_features=3, out_features=512)\n",
      "  (relu1): LeakyReLU(0.4, inplace)\n",
      "  (discriminator_L2): Linear(in_features=512, out_features=1)\n",
      "  (sigmo1): Sigmoid()\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] discriminator architecture:\\n\\n{}\\n'.format(now, discriminator_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Autoencoder Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the learning rate of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define autoencoder reconstruction loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "rec_loss = nn.BCELoss(size_average=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define autoencoder (encoder and decoder net) parameter optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning rate and optimization strategy\n",
    "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define regularization and generation loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the regularization loss / generation loss function\n",
    "reg_loss = nn.BCELoss(size_average=True)\n",
    "gen_loss = nn.BCELoss(size_average=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define generator net (equals encoder net) optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimization criterion and optimizer - generative net\n",
    "encoder_generator_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define discriminator net optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer = torch.optim.Adam(discriminator_train.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Adversarial Autoencoder Neural Network (AENN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train our deep autoencoder neural network (as implemented in section 4. of the lab) using the encoded transactional data (created in section 3. of the lab). More specifically, we will have a detailed look into the distinct training steps as well as how to monitor the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Preparing the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have pre-processed the dataset, implemented the AENN and defined the reconstruction error. Let's now start to train a corresponding model for **5 epochs** and a **mini-batch size of 128** journal entries per batch. This implies that the whole dataset will be fed to the AENN 5 times in chunks of 128 journal entries yielding to 4,165 mini-batches (533,009 journal entries / 128 journal entries per mini-batch) per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training parameters\n",
    "num_epochs = 10\n",
    "mini_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training phase, we will fetch the individual mini-batches of the entire population of journal entries. To achieve this, we will use PyTorch's `DataLoader` that provides single- or multi-process iterators over a given dataset to load one mini-batch at a time. By enabling `shuffle=True` the data will be reshuffled at every epoch prior to feeding it to the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pre-processed data to pytorch tensor\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "# note: we set num_workers to zero to retrieve deterministic results\n",
    "\n",
    "# determine if CUDA is available at compute node\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Running the Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:31:01] epoch: [0001/0010], batch: 0010, reconstruction loss: 0.2496831268\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:31:06] epoch: [0001/0010], batch: 0010, discrimination loss: 0.9454929829 [r_0.5459252000/f_0.3995678127]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:31:06] epoch: [0001/0010], batch: 0010, generator loss: 1.1418374777\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:31:50] epoch: [0001/0010], batch: 0020, reconstruction loss: 0.1594869494\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:31:55] epoch: [0001/0010], batch: 0020, discrimination loss: 0.7628086805 [r_0.4380775392/f_0.3247311413]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:31:55] epoch: [0001/0010], batch: 0020, generator loss: 1.3210605383\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:32:39] epoch: [0001/0010], batch: 0030, reconstruction loss: 0.1093036160\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:32:44] epoch: [0001/0010], batch: 0030, discrimination loss: 0.6835069656 [r_0.3911794722/f_0.2923274934]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:32:44] epoch: [0001/0010], batch: 0030, generator loss: 1.4321355820\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:33:29] epoch: [0001/0010], batch: 0040, reconstruction loss: 0.0865446106\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:33:34] epoch: [0001/0010], batch: 0040, discrimination loss: 0.7703620195 [r_0.4489254951/f_0.3214364946]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:33:34] epoch: [0001/0010], batch: 0040, generator loss: 1.3707555532\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:34:19] epoch: [0001/0010], batch: 0050, reconstruction loss: 0.0785885528\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:34:24] epoch: [0001/0010], batch: 0050, discrimination loss: 1.0360445976 [r_0.6183429360/f_0.4177016020]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:34:24] epoch: [0001/0010], batch: 0050, generator loss: 1.1526129246\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:35:09] epoch: [0001/0010], batch: 0060, reconstruction loss: 0.0726260841\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:35:14] epoch: [0001/0010], batch: 0060, discrimination loss: 1.3582295179 [r_0.8406728506/f_0.5175566673]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:35:14] epoch: [0001/0010], batch: 0060, generator loss: 0.9647451043\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:36:00] epoch: [0001/0010], batch: 0070, reconstruction loss: 0.0579567626\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:36:05] epoch: [0001/0010], batch: 0070, discrimination loss: 1.2397376299 [r_0.8733639121/f_0.3663737178]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:36:05] epoch: [0001/0010], batch: 0070, generator loss: 1.2323820591\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:37:01] epoch: [0001/0010], batch: 0080, reconstruction loss: 0.0484788530\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:37:07] epoch: [0001/0010], batch: 0080, discrimination loss: 0.9098659158 [r_0.5892559290/f_0.3206099868]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:37:07] epoch: [0001/0010], batch: 0080, generator loss: 1.3602751493\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:38:04] epoch: [0001/0010], batch: 0090, reconstruction loss: 0.0481315143\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:38:10] epoch: [0001/0010], batch: 0090, discrimination loss: 0.8953701854 [r_0.5122091174/f_0.3831610680]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:38:10] epoch: [0001/0010], batch: 0090, generator loss: 1.2772841454\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:39:07] epoch: [0001/0010], batch: 0100, reconstruction loss: 0.0534413569\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:39:12] epoch: [0001/0010], batch: 0100, discrimination loss: 1.1665693521 [r_0.6863018274/f_0.4802674949]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:39:12] epoch: [0001/0010], batch: 0100, generator loss: 1.0891062021\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:40:05] epoch: [0001/0010], batch: 0110, reconstruction loss: 0.0544868186\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:40:10] epoch: [0001/0010], batch: 0110, discrimination loss: 1.4749059677 [r_0.9136132598/f_0.5612927079]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:40:10] epoch: [0001/0010], batch: 0110, generator loss: 0.9185774922\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:41:01] epoch: [0001/0010], batch: 0120, reconstruction loss: 0.0480440855\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:41:06] epoch: [0001/0010], batch: 0120, discrimination loss: 1.6087957621 [r_1.0307089090/f_0.5780868530]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:41:06] epoch: [0001/0010], batch: 0120, generator loss: 0.8474743366\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:41:58] epoch: [0001/0010], batch: 0130, reconstruction loss: 0.0392176285\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:42:04] epoch: [0001/0010], batch: 0130, discrimination loss: 1.5179851055 [r_0.8979968429/f_0.6199882030]\n",
      "-----------------\n",
      "[PT LOG TRAIN 20180511-15:42:04] epoch: [0001/0010], batch: 0130, generator loss: 0.7884940505\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "[PT LOG TRAIN 20180511-15:42:54] epoch: [0001/0010], batch: 0140, reconstruction loss: 0.0338904671\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# init collection of training losses\n",
    "reconstruction_losses = []\n",
    "discrimination_losses_fake = []\n",
    "discrimination_losses_real = []\n",
    "generation_losses = []\n",
    "\n",
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# train adversarial autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "\n",
    "    if (torch.backends.cudnn.version() != None) and (use_cuda == True):\n",
    "\n",
    "        # set all networks / models in GPU mode\n",
    "        encoder_train.cuda()\n",
    "        decoder_train.cuda()\n",
    "        discriminator_train.cuda()\n",
    "\n",
    "    # set networks in training mode (apply dropout when needed)\n",
    "    encoder_train.train()\n",
    "    decoder_train.train()\n",
    "    discriminator_train.train()\n",
    "    \n",
    "    # start timer\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # iterate over epoch mini batches\n",
    "    for mini_batch_data in dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        # convert mini batch to torch variable\n",
    "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
    "\n",
    "        # reset the networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "        # =================== reconstruction phase =====================\n",
    "        \n",
    "        # =================== encoder and decoder training\n",
    "        \n",
    "        # run autoencoder encoding - decoding\n",
    "        z_sample = encoder_train(mini_batch_torch)\n",
    "        mini_batch_reconstruction = decoder_train(z_sample)\n",
    "\n",
    "        # determine reconstruction loss\n",
    "        reconstruction_loss = rec_loss(input=mini_batch_reconstruction, target=mini_batch_torch)\n",
    "\n",
    "        # run backward pass - determine gradients\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        # collect reconstruction loss\n",
    "        reconstruction_losses.extend([reconstruction_loss.data[0]])\n",
    "\n",
    "        # update network parameter - decoder and encoder\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        if mini_batch_count % 10 == 0:\n",
    "\n",
    "            print('------------------------------------------')\n",
    "            # print mini batch reconstuction results\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            print('[PT LOG TRAIN {}] epoch: [{:04}/{:04}], batch: {:04}, reconstruction loss: {:.10f}'.format(now, epoch + 1, num_epochs, mini_batch_count, reconstruction_loss.data[0]))\n",
    "            # print('[PT LOG TRAIN {}] epoch: [{:04}/{:04}], dec L1 mean: {:.8f}, dec L2 mean: {:.8f}, dec L3 mean: {:.8f}'.format(now, epoch + 1, num_epochs, torch.mean(decoder.decoder_L1.weight).data[0], torch.mean(decoder.decoder_L2.weight).data[0], torch.mean(decoder.decoder_L3.weight).data[0]))\n",
    "            print('-----------------')\n",
    "\n",
    "        # =================== regularization phase =====================\n",
    "\n",
    "        # =================== discriminator training\n",
    "\n",
    "        # set discriminator in evaluation mode\n",
    "        discriminator_train.eval()\n",
    "\n",
    "        # set z dimension to be sampled from\n",
    "        z_dim = 3\n",
    "        \n",
    "        # sample randomly from gaussian distribution -> real gaussian sample\n",
    "        z_real_gauss = autograd.Variable(torch.randn(torch_dataset.size()[0], z_dim) - 0.0)\n",
    "\n",
    "        if (torch.backends.cudnn.version() != None) and (use_cuda == True):\n",
    "\n",
    "            z_real_gauss = z_real_gauss.cuda()\n",
    "\n",
    "        # determine mini batch sample generated by the encoder -> fake gaussian sample\n",
    "        z_fake_gauss = encoder_train(mini_batch_torch)\n",
    "\n",
    "        # determine discriminator classification of both samples\n",
    "        d_real_gauss = discriminator_train(z_real_gauss) # real sampled gaussian \n",
    "        d_fake_gauss = discriminator_train(z_fake_gauss) # fake created gaussian\n",
    "\n",
    "        # determine discriminator classification target variables\n",
    "        d_real_gauss_target = autograd.Variable(torch.ones(d_real_gauss.data.size())) # real -> 1\n",
    "        d_fake_gauss_target = autograd.Variable(torch.zeros(d_fake_gauss.data.size())) # fake -> 0\n",
    "\n",
    "        if (torch.backends.cudnn.version() != None) and (use_cuda == True):\n",
    "\n",
    "            d_real_gauss_target = d_real_gauss_target.cuda()\n",
    "            d_fake_gauss_target = d_fake_gauss_target.cuda()\n",
    "\n",
    "        # determine individual discrimination losses\n",
    "        discrimination_loss_real = reg_loss(target=d_real_gauss_target, input=d_real_gauss) # real loss\n",
    "        discrimination_loss_fake = reg_loss(target=d_fake_gauss_target, input=d_fake_gauss) # fake loss\n",
    "\n",
    "        # collection discrimination losses\n",
    "        discrimination_losses_real.extend([discrimination_loss_real.data[0]])\n",
    "        discrimination_losses_fake.extend([discrimination_loss_fake.data[0]])\n",
    "        \n",
    "        # add real loss and fake loss\n",
    "        discrimination_loss = discrimination_loss_fake + discrimination_loss_real\n",
    "\n",
    "        # run backward through the discriminator network\n",
    "        discrimination_loss.backward()\n",
    "\n",
    "        # update network the discriminator network parameters\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # reset the networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "        if mini_batch_count % 10 == 0:\n",
    "\n",
    "            # print mini batch reconstuction results\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            print('[PT LOG TRAIN {}] epoch: [{:04}/{:04}], batch: {:04}, discrimination loss: {:.10f} [r_{:.10f}/f_{:.10f}]'.format(now, epoch + 1, num_epochs, mini_batch_count, discrimination_loss.data[0], discrimination_loss_real.data[0], discrimination_loss_fake.data[0]))\n",
    "            print('-----------------')\n",
    "\n",
    "        # =================== generator training\n",
    "\n",
    "        # set encoder / generator in training mode\n",
    "        encoder_train.train()\n",
    "        \n",
    "        # reset the encoder / generator networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "\n",
    "        # determine fake gaussian sample generated by the encoder / generator\n",
    "        z_fake_gauss = encoder_train(mini_batch_torch)\n",
    "\n",
    "        # determine discriminator classification of fake gaussian sample\n",
    "        d_fake_gauss = discriminator_train(z_fake_gauss)\n",
    "\n",
    "        # determine discriminator classification target variables\n",
    "        d_fake_gauss_target = autograd.Variable(torch.ones(d_fake_gauss.data.size())) # fake -> 1\n",
    "\n",
    "        if (torch.backends.cudnn.version() != None) and (use_cuda == True):\n",
    "\n",
    "            d_fake_gauss_target = d_fake_gauss_target.cuda()\n",
    "\n",
    "        # determine discrimination loss of fake gaussian sample\n",
    "        generation_loss = gen_loss(target=d_fake_gauss_target, input=d_fake_gauss)\n",
    "        \n",
    "        # collect generation losses\n",
    "        generation_losses.extend([generation_loss.data[0]])\n",
    "\n",
    "        # run backward pass - determine gradients\n",
    "        generation_loss.backward()\n",
    "\n",
    "        # update network paramaters - encoder / generatorc\n",
    "        encoder_generator_optimizer.step()\n",
    "\n",
    "        # reset the networks gradients\n",
    "        encoder_train.zero_grad()\n",
    "        decoder_train.zero_grad()\n",
    "        discriminator_train.zero_grad()\n",
    "\n",
    "        if mini_batch_count % 10 == 0:\n",
    "\n",
    "            # print mini batch reconstuction results\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            print('[PT LOG TRAIN {}] epoch: [{:04}/{:04}], batch: {:04}, generator loss: {:.10f}'.format(now, epoch + 1, num_epochs, mini_batch_count, generation_loss.data[0]))\n",
    "            #print('[PT LOG TRAIN {}] epoch: [{:04}/{:04}], enc L1 mean: {:.8f}, enc L2 mean: {:.8f}, enc L3 mean: {:.8f}'.format(now, epoch + 1, num_epochs, torch.mean(encoder.encoder_L1.weight).data[0], torch.mean(encoder.encoder_L2.weight).data[0], torch.mean(encoder.encoder_L3.weight).data[0]))\n",
    "            print('------------------------------------------')\n",
    "\n",
    "    # =================== save model snapshots to disk ============================\n",
    "    \n",
    "    # save trained encoder model file to disk\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "    encoder_model_name = \"{}_ep_{}_encoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(\"./models\", encoder_model_name))\n",
    "\n",
    "    # save trained decoder model file to disk\n",
    "    decoder_model_name = \"{}_ep_{}_decoder_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(\"./models\", decoder_model_name))\n",
    "    \n",
    "    # save trained discriminator model file to disk\n",
    "    decoder_model_name = \"{}_ep_{}_discriminator_model.pth\".format(now, (epoch+1))\n",
    "    torch.save(discriminator_train.state_dict(), os.path.join(\"./models\", decoder_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate if the loss function is indeed going down with progressing training of the model. Therefore, let's visualize the magnitudes of the losses obtained per training epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training progress\n",
    "plt.plot(range(0, len(losses)), losses)\n",
    "plt.xlabel('[training epoch]')\n",
    "plt.xlim([0, len(losses)])\n",
    "plt.ylabel('[reconstruction-error]')\n",
    "#plt.ylim([0.0, 1.0])\n",
    "plt.title('AENN training performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the reconstruction loss change as we progress in training our model? After 5 epochs, we can observe that our reconstruction loss already went down significantly and starts to converge nicely. This indicates that our network did a pretty good job in learning the structure and attributes of the journal entries.\n",
    "\n",
    "But, from the plot we also observe that the model could probably be trained a couple more epochs as the trend of the reconstruction error still decreases for the last few epochs. In order to save time, we will continue the lab using a pre-trained model already trained by 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with the next notebook section it's time for some exercises:\n",
    "\n",
    ">1. Set the `USE_CUDA` flag to `False` and re-start the training procedure. What impact do you observe in terms of training time needed for training a single epoch? Please, note that after altering the `USE_CUDA` flag you need to execute all successive cells starting from section 4.2. [5-10 min]\n",
    ">2. Set the `dropout` probability to `0.8` (80%) and re-start the training procedure. What impact do you observe in terms of training performance / reconstruction loss? Please, note that after altering the `dropout` probability you need to execute all successive cells starting from section 4.2. [5-10 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Autoencoder Neural Network (AENN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to train our autoencoder model, we will explore how we can use it to detect anomalies within the entire population of journal entries. Initially, we will start by loading a pre-trained model of 20 epochs and assess its reconstruction capability on the entire dataset. \n",
    "\n",
    "The pre-trained model is stored in the same directory as the lab notebook and can be loaded by executing the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore pretrained model checkpoint\n",
    "encoder_model_name = \"20180322-12_26_40_ep_20_encoder_model.pth\"\n",
    "decoder_model_name = \"20180322-12_26_40_ep_20_decoder_model.pth\"\n",
    "\n",
    "# init training network classes / architectures\n",
    "encoder_eval = encoder()\n",
    "decoder_eval = decoder()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(os.path.join(\"models\", encoder_model_name)))\n",
    "decoder_eval.load_state_dict(torch.load(os.path.join(\"models\", decoder_model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Assessment of the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once obtained, let's use the model to reconstruct the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# set networks in evaluation mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# reconstruct encoded transactional data\n",
    "reconstruction = decoder_eval(encoder_eval(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assess its quality by calculating the reconstruction error over the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine reconstruction loss - all transactions\n",
    "reconstruction_loss_all = loss_function(reconstruction, data)\n",
    "\n",
    "# print reconstruction loss - all transactions\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\n",
    "print('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, nice. Comparing the overall reconstruction loss of the pre-trained model trained for **20 epochs** to the one we initially trained for **5 epochs** reveals, that the pre-trained model results in a significantly lower reconstruction error. We can therefore conclude that the pre-trained model outperforms our initial model in capturing the inherent characteristics of the journal entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Assessment of the Individual Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we are convinced that the pre-trained model is of decent quality let's assess the individual journal entries of the dataset. To achieve this, we collect the reconstruction errors of each individual journal entry by executing the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init binary cross entropy errors\n",
    "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
    "\n",
    "# iterate over all detailed reconstructions\n",
    "for i in range(0, reconstruction.size()[0]):\n",
    "\n",
    "    # determine reconstruction loss - individual transactions\n",
    "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).data[0]\n",
    "\n",
    "    if(i % 100000 == 0):\n",
    "\n",
    "        ### print conversion summary\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have collected individual reconstruction errors let's visualize them accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# assign unique id to transactions\n",
    "plot_data = np.column_stack((np.arange(len(reconstruction_loss_transaction)), reconstruction_loss_transaction))\n",
    "\n",
    "# obtain regular transactions as well as global and local anomalies\n",
    "regular_data = plot_data[label == 'regular']\n",
    "global_outliers = plot_data[label == 'global']\n",
    "local_outliers = plot_data[label == 'local']\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', alpha=0.4, marker=\"o\", label='regular') # plot regular transactions\n",
    "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"^\", label='global') # plot global outliers\n",
    "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C2', marker=\"^\", label='local') # plot local outliers\n",
    "\n",
    "# add plot legend of transaction classes\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization reveals that the pre-trained model is able to reconstruct the majority of regular journal entries, while failing to do so, for the anomalous ones. As a result, the model reconstruction error can be used to distinguish both \"global\" anomalies (orange) and \"local\" anomalies (green) from the regular journal entries (blue).\n",
    "\n",
    "To further investigate our observation and confirm the initial assumption, let's have a closer look into the journal entries exhibiting a \"high\" binary cross-entropy reconstruction error >= 0.1. We assume that these journal entries correspond to the \"global\" anomalies of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append labels to original dataset\n",
    "ori_dataset['label'] = label\n",
    "\n",
    "# inspect transactions exhibiting a reconstruction error >= 0.2\n",
    "ori_dataset[reconstruction_loss_transaction >= 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now also have a closer look into the journal entries exhibiting a \"medium\" binary cross-entropy reconstruction error >= 0.02 and < 0.1. We assume that these journal entries mostly correspond to the \"local\" anomalies of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect transactions exhibiting a reconstruction error < 0.1 and >= 0.05\n",
    "ori_dataset[(reconstruction_loss_transaction >= 0.05) & (reconstruction_loss_transaction < 0.1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read ahead and only come back to these optional exercises if time permits.\n",
    "\n",
    "**1. Train the autoencoder model from scratch** [15 mins]\n",
    "\n",
    "First, change the number of training epochs `num_epochs` to **30** in the corresponding cell of section 5.1. Second, you might also want to try different learning rates other than the initial learning rate of **0.001** in the corresponding cell of section 4.3. Third, comment out the two lines in the first cell of section 6. where the pre-trained model is defined (under \"restore pre-trained model checkpoint\") as well as two lines where it is loaded (under \"load trained models\"). \n",
    "\n",
    "Please, note that after altering the number of training epochs in section 5.1 and/or the learning rate in section 4.3 you need to execute all successive cells starting from section 5.1 or 4.3.\n",
    "\n",
    "**2. What would happen if we remove a few fully-connected layers?** [15 mins]\n",
    "\n",
    "We designed a specific model for the lab because experiments show that the structure provided result in a good detection accuracy. Let's see how the reconstruction performance change if we would **remove several of the hidden layers**. First, adjust the encoder and decoder model definitions in section 4.2 accordingly (you may want to use the code snippets shown below). Then, follow all the instructions for training from scratch.\n",
    "\n",
    "Please, note that after altering the encoder and / or decoder network architecture in section 4.2 you need to execute all successive cells starting from section 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the shallow encoder network \n",
    "# containing only a single layer\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 618, out 3\n",
    "        self.encoder_L1 = nn.Linear(in_features=618, out_features=3, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform(self.encoder_L1.weight) # init weights according to [9]\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.encoder_R1(self.encoder_L1(x)) # don't apply dropout to the AE bottleneck\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the shallow decoder network \n",
    "# containing only a single layer\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 3, out 618\n",
    "        self.decoder_L1 = nn.Linear(in_features=3, out_features=618, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform(self.decoder_L1.weight)  # init weights according to [9]\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.decoder_R1(self.decoder_L1(x)) # don't apply dropout to the AE output\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Explore the latent space representation** [15 min]\n",
    "\n",
    "In a real world scenario it is usually beneficial to visualize the data manifolds to:\n",
    "\n",
    "> 1. determine if the autoencoder learned a meaningful representation; and,\n",
    "> 2. obtain an impression of the structural characteristics and dependencies in the data.\n",
    "\n",
    "To achieve this, we will propagate the data through the trained model and capture for each transaction the respective representation in the latent space as referred to as \"embeddings\" (the activation pattern of each journal entry at the bottleneck neurons). \n",
    "\n",
    "Therefore, we intentionally chose a bottleneck layer comprised of 3 neurons. This provides us the ability to visualize the distinctive activation pattern using the 3d plotting capabilities of Pythons matplotlib library. In the subsequent cells we already prepared a plotting function that draws a 3d scatter plot of the latent space representation of each transaction at a particular epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot latent space representation of all samples given latent variable and label class\n",
    "def plot_latent_space(latent_variable, label, epoch):\n",
    "    \"\"\" Plots latent space activations as a 3d scatter plot at particular epoch\n",
    "    :param latent_space: activations of latent space\n",
    "    :param label: 1-d array of labels defining type of anomaly\n",
    "    :param epoch: training epoch\n",
    "    \"\"\"\n",
    "    # prepare plot\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.view_init(30, 45)\n",
    "\n",
    "    # set axis paramaters of subplot\n",
    "    ax.grid(linestyle='dotted')\n",
    "    \n",
    "    # set label and title details\n",
    "    ax.set_xlabel(r'activation [$z_1$]', weight='normal', fontsize=12)\n",
    "    ax.set_ylabel(r'activation [$z_2$]', weight='normal', fontsize=12)\n",
    "    ax.set_zlabel(r'activation [$z_3$]', weight='normal', fontsize=12)\n",
    "    plt.title('latent space activations at epoch ' + str(epoch), fontsize=12)\n",
    "\n",
    "    # plot regular transactions\n",
    "    regular = latent_variable[np.where(label == 'regular')]\n",
    "    ax.scatter(regular[:, 0], regular[:, 1], regular[:, 2], c='C0', alpha=0.4, marker=\"o\")\n",
    "\n",
    "    # plot first order anomalous transactions\n",
    "    anomalies_1 = latent_variable[np.where(label == 'global')]\n",
    "    ax.scatter(anomalies_1[:, 0], anomalies_1[:, 1], anomalies_1[:, 2], c='C1', s=100, marker=\"^\")\n",
    "\n",
    "    # plot second order anomalous transactions\n",
    "    anomalies_2 = latent_variable[np.where(label == 'local')]\n",
    "    ax.scatter(anomalies_2[:, 0], anomalies_2[:, 1], anomalies_2[:, 2], c='C2', s=100, marker=\"^\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before plotting the embeddings we load the model of the shallow autoencoder network you have been asked to train in exercise 7.2 of the optional exercises. Since we are interested on the latent space representation of each transaction we only need to load the pre-trained encoder part of the autoencoder (ignoring the decoder part) and propagate all transactions through it.\n",
    "\n",
    "The function below will accomplish these steps for a given pre-trained encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract latent space representation of all samples given the name of encoder model to be loaded\n",
    "def get_latent_space(encoder_model_name):\n",
    "    ''' Extracts the latent space representation\n",
    "    :param encoder_model_name: file name of the pretrained encoder model\n",
    "    :return: latent space representation\n",
    "    '''\n",
    "    # init training network classes / architectures\n",
    "    encoder_eval = encoder()\n",
    "\n",
    "    # load trained models\n",
    "    encoder_eval.load_state_dict(torch.load(os.path.join(\"models\", encoder_model_name)))\n",
    "\n",
    "    # convert encoded transactional data to torch Variable\n",
    "    data = autograd.Variable(torch_dataset)\n",
    "\n",
    "    # set networks in training mode (don't apply dropout)\n",
    "    encoder_eval.eval()\n",
    "\n",
    "    # extract encoded latent space representation\n",
    "    latent_variable = encoder_eval(data).data.numpy()\n",
    "    \n",
    "    return latent_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how the latent space representation \"embeddings\" of the individual transactions changes with progressing training epochs. In order to do so, you can load any checkpoint of a particular training epoch and visualize the latent representations you want to have a look at using the code snipped below. \n",
    "\n",
    "If you didn't find the time to accomplish exercise 7.2 you may want to plot the embeddings using the pre-trained model we prepared for this lab as stated in the cell below. This will also give you an idea of the transactional manifolds learned by the autoencoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect latent space representation at a certain epochs \n",
    "z_representation = get_latent_space(encoder_model_name='20180321-13_51_52_ep_10_encoder_model.pth')\n",
    "\n",
    "# plot the latent space at a particular epoch\n",
    "plot_latent_space(z_representation, label, epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you may want to save the content of the lab to your local machine. Therefore, please execute the cell below to retrieve an archived version of your current lab content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf L8113.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to execute the content of your lab outside of the jupyter notebook environment e.g. on compute node or server. The cell below converts the lab notebook into a standalone and executable python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script GTC_2018_Lab.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lab Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we presented a step by step implementation of an autoencoder deep neural network based methodology to detect anomalies in financial data. The degree of a financial transaction \"abnormity\" is evaluated based on its respective reconstruction error. The code provided in this lab can be tailored to meet more complex fraud detection scenarios and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Post-Lab Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following exercises after the lab:\n",
    "\n",
    "**1. Evaluation of shallow and deep autoencoder models** \n",
    "\n",
    "Try to train and evaluate further (shallow and deeper) autoencoder models (by removing and adding of fully-connected layers). Analyse the performance in terms of training time and reconstruction error.\n",
    "\n",
    "**2. Comparison to other dimensionality reduction techniques**\n",
    "\n",
    "Try using other dimensionality reduction techniques such as principal component analysis, non-negative matrix factorization or sparse coding and compare the detected anomalies with the ones detected by the autoencoder.\n",
    "\n",
    "**3. Review of additional autoencoder concepts**\n",
    "\n",
    "Try using other autoencoder architectures such as variational [13] or adversarial [14] autoencoder and compare the results with the autoencoder architecture implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 10. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Major elements of the lab content are inspired by the publication \"Detection of Anomalies in Large Scale Accounting Data using Deep Autoencoder Networks\", of M. Schreyer, T. Sattarov, D. S. Borth, A. Dengel, and B. Reimer, 2017 (arXiv preprint available under: https://arxiv.org/abs/1709.05254).\n",
    "\n",
    "[1] ACFE, \"Report to the Nations on Occupational Fraud and Abuse\", The 2016 Global Fraud Study, Association of Certified Fraud Examiners (ACFE), 2016.\n",
    "\n",
    "[2] J. T. Wells, \"Corporate Fraud Handbook: Prevention and Detection\", John Wiley & Sons, 2017.\n",
    "\n",
    "[3] PwC, \"Pulling Fraud Out of the Shadows\", The Global Economic Crime and Fraud Survey 2018, PricewaterhouseCoopers LLP, 2018.\n",
    "\n",
    "[4] S. Markovitch, P. Willmott, \"Accelerating the digitization of business processes\", McKinsey & Company (2014) 15.\n",
    "\n",
    "[5] SAP, SAP Global Corporate Affairs, Corporate Factsheet 2017, 2017.\n",
    "\n",
    "[6] E. A. Lopez-Rojas , A. Elmir, and S. Axelsson, \"PaySim: A financial mobile money simulator for fraud detection\", In: The 28th European Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus, 2016.\n",
    "\n",
    "[7] G. E. Hinton, and R. R. Salakhutdinov, \"Reducing the dimensionality of data with neural networks\", science 313, no. 5786: 504-507, 2006.\n",
    "\n",
    "[8] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: A simple way to prevent neural networks from overfitting\", The Journal of Machine Learning Research, 15(1), 1929-1958, 2014.\n",
    "\n",
    "[9] X. Glorot and Y. Bengio, \"Understanding the difficulty of training deep feedforward neural networks\", Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), 9:249256, 2010.\n",
    "\n",
    "[10] B. Xu, N. Wang, T. Chen, and M. Li, \"Empirical Evaluation of Rectified Activations in Convolution Network\", ICML Deep Learning Workshop, pages 15, 2015.\n",
    "\n",
    "[11] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization\", International Conference on Learning Representations (ICLR). 2015.\n",
    "\n",
    "[12] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. R. Salakhutdinov. \"Improving neural networks by preventing co-adaptation of feature detectors\", Technical Report, 2012.\n",
    "\n",
    "[13] D. P. Kingma, M. Welling. \"Auto-encoding variational bayes\", arXiv preprint arXiv:1312.6114, 2013.\n",
    "\n",
    "[14] Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., & Frey, B., \"Adversarial autoencoders\", arXiv preprint arXiv:1511.05644, 2015.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
